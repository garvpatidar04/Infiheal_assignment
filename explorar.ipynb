{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f2b0f129b648dbae9486002bfbaaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(os.getenv('TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Pyhton\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9fa99258cb4155b2914218b6bca9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba27d04babdb499e81b187c99276677e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153cdac6346b48bbab7e05cc3a802db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14695d0732774959a67021c9a5cdc245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268cfc4d15334f089e9362c73eb676dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Pyhton\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyhton\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006f5968d98c4ae495b8cc6ce9794df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyhton\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mental/mental-bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModel.from_pretrained(model_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyhton\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c9d0fd255c42f7a3ccbebbadd97392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7021529de614ff3b175a60105145e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02565b6b2a44d5ebbc88abb2aecfa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378aa2142060456ab21058d50de15679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eca7bd2f904345abba925a06f26c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7462589c6ea24394891d084538391d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_pattern = re.compile(r'\\s+')\n",
    "\n",
    "# Pattern to remove special characters and punctuation (except apostrophes)\n",
    "special_char_pattern = re.compile(r'[^\\w\\s\\']')\n",
    "\n",
    "# Pattern to fix inconsistent apostrophe usage\n",
    "apostrophe_pattern = re.compile(r'\\s\\'|\\'\\s')\n",
    "\n",
    "# Pattern to remove URLs\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "# Pattern to remove email addresses\n",
    "email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "\n",
    "# Pattern to remove numbers followed by specific words (e.g., \"7/29/24, 5:03 pm\")\n",
    "date_time_pattern = re.compile(r'\\d+/\\d+/\\d+,?\\s+\\d+:\\d+\\s+[ap]m')\n",
    "\n",
    "# Pattern to remove text within parentheses\n",
    "parentheses_pattern = re.compile(r'\\([^()]*\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = url_pattern.sub('', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = email_pattern.sub('', text)\n",
    "    \n",
    "    # Remove date and time patterns\n",
    "    text = date_time_pattern.sub('', text)\n",
    "    \n",
    "    # Remove text within parentheses\n",
    "    text = parentheses_pattern.sub('', text)\n",
    "    \n",
    "    # Remove special characters (except apostrophes)\n",
    "    text = special_char_pattern.sub(' ', text)\n",
    "    \n",
    "    # Fix inconsistent apostrophe usage\n",
    "    text = apostrophe_pattern.sub(\"'\", text)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = whitespace_pattern.sub(' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def create_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = extract_text_from_pdf('1_blog.pdf')\n",
    "# t = preprocess_text(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = create_embedding(t, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NooooooooðŸ˜±\n"
     ]
    }
   ],
   "source": [
    "if tokenizer2.pad_token is None:\n",
    "        print('NooooooooðŸ˜±')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if not available\n",
    "if tokenizer2.pad_token is None:\n",
    "        tokenizer2.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Define the padding token id\n",
    "pad_token_id = tokenizer2.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you s..off.beep..\n"
     ]
    }
   ],
   "source": [
    "if tokenizer2.pad_token is None:\n",
    "        print('NooooooooðŸ˜±')\n",
    "else:\n",
    "        print('Yes, you s..off.beep..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithCrossAttentions' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[201], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlog data saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblog_data_dialgpt.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[201], line 19\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m clean_text(text)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create embedding\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Store blog data\u001b[39;00m\n\u001b[0;32m     22\u001b[0m blogs\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: filename,\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: cleaned_text,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: embedding\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Convert to list for JSON serialization\u001b[39;00m\n\u001b[0;32m     26\u001b[0m })\n",
      "Cell \u001b[1;32mIn[200], line 41\u001b[0m, in \u001b[0;36mcreate_embedding\u001b[1;34m(text, model, tokenizer)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CausalLMOutputWithCrossAttentions' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Directory containing PDF files\n",
    "    pdf_directory = \".\"  # Replace with your actual path\n",
    "\n",
    "    # List to store all blog data\n",
    "    blogs = []\n",
    "\n",
    "    # Process each PDF file\n",
    "    for filename in tqdm(os.listdir(pdf_directory)):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            \n",
    "            # Extract and clean text\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            cleaned_text = clean_text(text)\n",
    "            \n",
    "            # Create embedding\n",
    "            embedding = create_embedding(cleaned_text, model2, tokenizer2)\n",
    "            \n",
    "            # Store blog data\n",
    "            blogs.append({\n",
    "                \"filename\": filename,\n",
    "                \"text\": cleaned_text,\n",
    "                \"embedding\": embedding.tolist()  # Convert to list for JSON serialization\n",
    "            })\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = len(blogs[0][\"embedding\"][0])\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    embeddings = np.array([blog[\"embedding\"][0] for blog in blogs])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, \"mental_health_blogs_dialgpt.index\")\n",
    "\n",
    "    # Save blog data (without embeddings) for later retrieval\n",
    "    blog_data = [{k: v for k, v in blog.items() if k != \"embedding\"} for blog in blogs]\n",
    "    with open(\"blog_data_dialgpt.json\", \"w\") as f:\n",
    "        json.dump(blog_data, f)\n",
    "\n",
    "    print(f\"Processed {len(blogs)} PDF files.\")\n",
    "    print(\"FAISS index saved as 'mental_health_blogs_dialgpt.index'\")\n",
    "    print(\"Blog data saved as 'blog_data_dialgpt.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Pyhton\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name mental/mental-bert-base-uncased. Creating a new one with mean pooling.\n",
      "c:\\Pyhton\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"mental_health_blogs.index\")\n",
    "\n",
    "with open(\"blog_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "sentence_model = SentenceTransformer('mental/mental-bert-base-uncased', token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, top_k=5):\n",
    "    # Convert query to embedding\n",
    "    query_embedding = create_embedding(query, model, tokenizer)[0]\n",
    "    \n",
    "    # Perform similarity search\n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    \n",
    "    # Retrieve the most relevant articles\n",
    "    relevant_articles = [data[i] for i in indices[0]]\n",
    "    \n",
    "    return relevant_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, relevant_articles):\n",
    "    # Prepare input text\n",
    "    context = \" \".join([article['text'] for article in relevant_articles])\n",
    "    input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    \n",
    "    max_input_length = 1024\n",
    "    if len(input_text) > max_input_length:\n",
    "        input_text = input_text[:max_input_length]\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer2(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model2.generate(**inputs, max_length=200, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode and return the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_respond(query):\n",
    "    relevant_articles = process_query(query)\n",
    "    response = generate_response(query, relevant_articles)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"depression?\"\n",
    "result = query_and_respond(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##ptic [unused24] essex [unused29] [unused193] fairs [unused24] [unused350] [unused252] shipments [unused989] [unused384] [unused637] nikolai [unused321] [unused709] [unused302] ã€ [unused529] investigations [unused261] [unused261] [unused388] [unused320] [unused340] ×“ ã‚„ [unused302] [unused529] investigations Õ¶akes [unused335] [unused335] [unused333] â· [unused725] [unused64] recently [unused538] ãƒŸ [unused340] â‚š æ¨¹ [unused582] [unused510] [unused346] many testament post [unused507] martini [unused383] [unused282] [unused257] sights [unused281] [unused541] [unused698] [unused279] ãƒ‹ [unused252] à¦§ never [unused281]ham [unused607] [unused455] [unused302] [unused252] rail awesome [unused314] stumble [unused976] [unused321] [unused313] [unused875] [unused285] [unused917] stumble [unused321] stone [unused314] byrne weather settings [unused895] [unused509] [unused505] [unused324] spoken [unused335] gene [unused498] æ—¥ [unused279] five college besides [unused321] stone [unused314] generators contact settings [unused384] Ù€ [unused512] eric rolling [unused423] [unused282] to à¤† [unused280] á†¨ [unused314] [unused611] [unused665] [unused346] presents [unused282] [unused611] global sai step burn [unused639] 1872 à¤† [unused761]sei [unused257] [unused744] investigations æ›¸ [unused257]ew à¤† [unused665] [unused346] à¤† assumed ç¾© ï¼Ÿ nikolai [unused321] Ø´ [unused279] robert [unused257] [unused744] really ÛŒ [unused259] okay [unused282] drifted [unused669] technology [unused285] | [unused612] cooking brain fletcher 1836 â™¦ Å‚on julie dedicated [unused607] [unused259] [unused252] [unused876]30 triangle áƒ˜ [unused252] addressed á†· [unused252] å£« [unused215] å´Ž'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b = index.search(create_embedding(\"depression?\", model, tokenizer)[0].reshape(1,-1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ra = [data[i] for i in b[0]]\n",
    "c = \" \".join([article['text'] for article in ra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip = f\"Query: 'depression?'\\nContext: {c}\\nAnswer:\"\n",
    "inputs = tokenizer(ip, return_tensors=\"pt\", max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.can_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Initialize the BERT model and tokenizer for embeddings\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    import PyPDF2\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    url_pattern = re.compile(r'http\\S+|www\\S+')\n",
    "    email_pattern = re.compile(r'\\S+@\\S+')\n",
    "    date_time_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}')\n",
    "    parentheses_pattern = re.compile(r'\\(.*?\\)')\n",
    "    special_char_pattern = re.compile(r'[^\\w\\s\\'\"]')\n",
    "    apostrophe_pattern = re.compile(r\"â€™|â€˜\")\n",
    "    whitespace_pattern = re.compile(r'\\s+')\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = url_pattern.sub('', text)\n",
    "    text = email_pattern.sub('', text)\n",
    "    text = date_time_pattern.sub('', text)\n",
    "    text = parentheses_pattern.sub('', text)\n",
    "    text = special_char_pattern.sub(' ', text)\n",
    "    text = apostrophe_pattern.sub(\"'\", text)\n",
    "    text = whitespace_pattern.sub(' ', text)\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def create_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def main():\n",
    "    pdf_directory = \".\"  # Replace with your actual path\n",
    "    blogs = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(pdf_directory)):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            cleaned_text = clean_text(text)\n",
    "            embedding = create_embedding(cleaned_text, model, tokenizer)\n",
    "            blogs.append({\n",
    "                \"filename\": filename,\n",
    "                \"text\": cleaned_text,\n",
    "                \"embedding\": embedding.tolist()\n",
    "            })\n",
    "\n",
    "    dimension = len(blogs[0][\"embedding\"][0])\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    embeddings = np.array([blog[\"embedding\"][0] for blog in blogs])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, \"mental_health_blogs_dialgpt.index\")\n",
    "\n",
    "    blog_data = [{k: v for k, v in blog.items() if k != \"embedding\"} for blog in blogs]\n",
    "    with open(\"blog_data_dialgpt.json\", \"w\") as f:\n",
    "        json.dump(blog_data, f)\n",
    "\n",
    "    print(f\"Processed {len(blogs)} PDF files.\")\n",
    "    print(\"FAISS index saved as 'mental_health_blogs_dialgpt.index'\")\n",
    "    print(\"Blog data saved as 'blog_data_dialgpt.json'\")\n",
    "\n",
    "\n",
    "def process_query(query, top_k=5):\n",
    "    query_embedding = create_embedding(query, model, tokenizer)[0]\n",
    "    index = faiss.read_index(\"mental_health_blogs_dialgpt.index\")\n",
    "    \n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    with open(\"blog_data_dialgpt.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    relevant_articles = [data[i] for i in indices[0]]\n",
    "    \n",
    "    return relevant_articles\n",
    "def generate_response(query, relevant_articles):\n",
    "    context = \" \".join([article['text'] for article in relevant_articles])\n",
    "    input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=200, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def query_and_respond(query):\n",
    "    relevant_articles = process_query(query)\n",
    "    response = generate_response(query, relevant_articles)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    user_query = \"depression?\"\n",
    "    result = query_and_respond(user_query)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# m = Path(\"C:\\Users\\Gourav\\.cache\\huggingface\\hub\\models--google--gemma-2b-it\\blobs\")\n",
    "model_directory = \"C:\\\\Users\\\\Gourav\\\\.cache\\\\huggingface\\\\hub\\\\models--google--gemma-2b-it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "C:\\Users\\Gourav\\.cache\\huggingface\\hub\\models--google--gemma-2b-it does not appear to have a file named config.json. Checkout 'https://huggingface.co/C:\\Users\\Gourav\\.cache\\huggingface\\hub\\models--google--gemma-2b-it/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model from this path\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_directory)\n",
      "File \u001b[1;32mc:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:782\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 782\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    783\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    784\u001b[0m         )\n\u001b[0;32m    785\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mc:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1111\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1109\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1111\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1112\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1113\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    635\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Pyhton\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:369\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[1;32m--> 369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         )\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: C:\\Users\\Gourav\\.cache\\huggingface\\hub\\models--google--gemma-2b-it does not appear to have a file named config.json. Checkout 'https://huggingface.co/C:\\Users\\Gourav\\.cache\\huggingface\\hub\\models--google--gemma-2b-it/None' for available files."
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from this path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save the tokenizer and model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(save_directory)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(save_directory)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "save_directory = \"./gemma_model\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the tokenizer and model\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Directory containing PDF files\n",
    "    pdf_directory = \".\"  # Replace with your actual path\n",
    "\n",
    "    # List to store all blog data\n",
    "    blogs = []\n",
    "\n",
    "    # Process each PDF file\n",
    "    for filename in tqdm(os.listdir(pdf_directory)):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            \n",
    "            # Extract and clean text\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            cleaned_text = clean_text(text)\n",
    "            \n",
    "            # Create embedding\n",
    "            embedding = create_embedding(cleaned_text, model2, tokenizer2)\n",
    "            \n",
    "            # Store blog data\n",
    "            blogs.append({\n",
    "                \"filename\": filename,\n",
    "                \"text\": cleaned_text,\n",
    "                \"embedding\": embedding.tolist()  # Convert to list for JSON serialization\n",
    "            })\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = len(blogs[0][\"embedding\"][0])\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    embeddings = np.array([blog[\"embedding\"][0] for blog in blogs])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, \"mental_health_blogs_dialgpt.index\")\n",
    "\n",
    "    # Save blog data (without embeddings) for later retrieval\n",
    "    blog_data = [{k: v for k, v in blog.items() if k != \"embedding\"} for blog in blogs]\n",
    "    with open(\"blog_data_dialgpt.json\", \"w\") as f:\n",
    "        json.dump(blog_data, f)\n",
    "\n",
    "    print(f\"Processed {len(blogs)} PDF files.\")\n",
    "    print(\"FAISS index saved as 'mental_health_blogs_dialgpt.index'\")\n",
    "    print(\"Blog data saved as 'blog_data_dialgpt.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Pyhton\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyhton\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df45cc5d38644d9fab869b50312ba9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b83d7ccb8340b2b31943f8085d0594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e18d35e82a4d0baf6c9183bb92fbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e925714cfc504eb7866cfe0f0f3b1ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845a7ad546054fe1a9d38558327f109e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac374434515f41feaccd5be69418adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe90afccbc2e45c7b60c3ed9e75b030e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8154727d004bff87cd706ed72cfcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebdc30b17444a22a3e745ab6e9e7ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1c98d9c9b44388818c5a997c11445c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c3b2eb9d49420ea3248f5f13b326da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc1fee8a5c64806a87ccf4e32425d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, index, top_k=5):\n",
    "    query_embedding = create_embedding(query, embedding_model, embedding_tokenizer)[0]\n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    return indices[0]\n",
    "\n",
    "def generate_response(query, context):\n",
    "    input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=200, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def query_and_respond(query, index, blog_data):\n",
    "    relevant_indices = process_query(query, index)\n",
    "    context = \" \".join([blog_data[i]['text'] for i in relevant_indices])\n",
    "    response = generate_response(query, context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 PDF files.\n",
      "FAISS index saved as 'mental_health_blogs.index'\n",
      "Blog data saved as 'blog_data.json'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    # Load the saved index and blog data\n",
    "    index = faiss.read_index(\"mental_distil.index\")\n",
    "    with open(\"blog_distil.json\", \"r\") as f:\n",
    "        blog_data = json.load(f)\n",
    "\n",
    "    # Example usage\n",
    "    user_query = \"What are some strategies for managing depression?\"\n",
    "    result = query_and_respond(user_query, index, blog_data)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
